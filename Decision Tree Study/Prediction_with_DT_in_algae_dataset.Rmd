---
title: "Prediction with decision tree method in algae dataset."
author: "Yan Yan"
date: "February 22, 2015"
output: pdf_document
---


```{r}
library(DMwR)
library(car)

#par(mfrow=c(1,2))
hist(algae$mxPH, prob=T, xlab='', main='Histogram of maixmum pH value', ylim=0:1)

# A smoothed version of hist graph
lines(density(algae$mxPH, na.rm=T)) 

#rug() used for plotting, jitter() randomized the original value to avoid overlap
rug(jitter(algae$mxPH)) 

#Q-Q graph, plot the scatterplot of value and Normal Distribution quantiles,
#and then the band chart of 95% confidence interval
qqPlot(algae$mxPH, main='Normal QQ plot of maximum pH') 
#par(mfrow=c(1,2))

#Boxplot of oPO4, the line in the middle of the box is the median, the upper 
#boudary is the 3rd quantiles and the lower boundary is the 1st quantile.
boxplot(algae$oPO4, ylab = "Orthophosphate(oPO4)")

rug(jitter(algae$oPO4), side=2)
#Plot the average value line in the Boxplot
abline(h = mean(algae$oPO4, na.rm = T), lty = 2)
```

We can see the the distribution of oPO4 is concentrated in the lower values, so it is positive-skewed.

For the sperated values, we can handle with the following method.
```{r}
plot(algae$NH4, xlab = " ")
abline(h = mean(algae$NH4, na.rm = T), lty = 1) #avarage value
abline(h = mean(algae$NH4, na.rm = T) + sd(algae$NH4, na.rm = T), lty = 2)
abline(h = median(algae$NH4, na.rm = T), lty = 3) #median
#A interact method can be realized by using identify() function
#identify(algae$NH4)
#clicked.lines <- identify(algae$NH4)
#algae[clicked.lines]
```
To find how the distribution varies due to other variables.

```{r}
library(lattice)
bwplot(size ~ a1, data=algae, ylab = 'River Size', xlab = 'Algal A1')
```

Also we can use the quantile box plot

```{r}
library(Hmisc)
bwplot(size ~ a1, data = algae, panel = panel.bpplot, 
       probs = seq(.01, .49, by = .01), datadensity = TRUE, 
       ylab = 'River Size', xlab = 'Algal A1')
```

We can also discretize the data into several intervals, which means transfer the continus numerical data into factor data, for example:

```{r}
minO2 <- equal.count(na.omit(algae$mnO2), number = 4, overlap = 1/5)
stripplot(season ~ a3|minO2, data=algae[!is.na(algae$mnO2),])
```
Then we go to the missing value process.
```{r}
#caculate the numbers of lines with missing value
algae[!complete.cases(algae),]
nrow(algae[!complete.cases(algae),])
#delete the lines with missing values
#algae <- na.omit(algae)

data(algae)
algae <- algae[-manyNAs(algae),]
algae <- centralImputation(algae)

#cor(algae[, 4:18], use = "complete.obs")
symnum(cor(algae[, 4:18], use = "complete.obs"))

data(algae)
algae <- algae[-manyNAs(algae),]
lm(PO4 ~ oPO4, data = algae)

algae[28, "PO4"] <- 42.897 + 1.293 * algae[28, "oPO4"]

data(algae)
algae <- algae[-manyNAs(algae),]
fillPO4 <- function(oP){
        if(is.na(oP))
                return(NA)
        else return(42.897 + 1.293 * oP)
}

algae[is.na(algae$PO4), "PO4"] <- sapply(algae[is.na(algae$PO4), "oPO4"], fillPO4)

histogram(~mxPH | season, data = algae)

algae$season <- factor(algae$season, levels = c("spring", "summer", "autumn", "winter"))
histogram(~mxPH | season, data = algae)

histogram(~mxPH | size * speed, data =algae)

stripplot(size ~ mxPH | speed, data = algae, jitter = T)
```

If we assume the two kinds of water were similar, then in one sample there were some missing value, the missing value might be similar to the correspond one in the other kind of water.

```{r}
data(algae)
algae <- algae[-manyNAs(algae),]

#algae <- knnImputation(algae, k = 10)
#or
algae <- knnImputation(algae, k = 10, meth = "median")
```

First we build a multivariate analysis for prediction
```{r}
data(algae)
algae <- algae[-manyNAs(algae),]
clean.algae <- knnImputation(algae, k = 10)

lm.a1 <- lm(a1 ~., data = clean.algae[,1:12])
summary(lm.a1)
```
Use anova() to simplify the model, when anova() was adopted into the simple linear model, this function provide a Sequential analysis of variance.

```{r}
library(stats)
anova(lm.a1)
```

From the result we can see that season contibute least for the error of fitting, then we delete it from the model.
```{r}
lm2.a1 <- update(lm.a1, .~ .-season)
summary(lm2.a1)
```

Then let's consider a comparison between model1 and model2.
```{r}
anova(lm.a1,lm2.a1)

```

From the F-test to the anova analysis of the 2 models, the two models were significant different. Then we continue to do the variable remove process. In R we use 

```{r}
final.lm <- step(lm.a1)
summary(final.lm)
```

From which we can see that the R-square is still not high, which means it's not appropriate for the prediction using multivariate analysis.
```{r}
library(rpart)
data(algae)
algae <- algae[-manyNAs(algae),]
rt.a1 <- rpart(a1 ~., data = algae[, 1:12])
prettyTree(rt.a1)
printcp(rt.a1)
```

Using *rpart()* to construct a tree would stop at some conditions were satisfied, like: the deviation less than a threshold; the samples number is smaller than a threshold; the deepth of the tree is larger than a threshold.
In *rpart()* the parameters were *cp*, *minsplit* and *madepth*. Using *printcp()* to show the results.

Use the treshold of cp = 0.08
```{r}
rt2.a1 <- prune(rt.a1, cp = 0.08)
rt2.a1

(rt.a1 <- rpartXse(a1 ~., data = algae[, 1:12]))
```

Using snip.part() to pruning interactively
```{r}
first.tree <- rpart(a1 ~., data = algae[,1:12])
snip.rpart(first.tree, c(4, 7))
prettyTree(first.tree)
#snip.rpart(firt.tree) interactively pruning
```

Prediction:
```{r}
lm.predictions.a1 <- predict(final.lm, clean.algae)
rt.predictions.a1 <- predict(rt.a1, algae)
```
The average error

```{r}
(mae.a1.lm <- mean(abs(lm.predictions.a1 - algae[, "a1"])))
(mae.a1.rt <- mean(abs(rt.predictions.a1 - algae[,"a1"])))

```
or the MSE

```{r}
(mse.a1.lm <- mean((lm.predictions.a1 - algae[, "a1"])^2))
(mse.a1.rt <- mean((rt.predictions.a1 - algae[, "a1"])^2))
```

The problems with the mse were that the not unified units so that it's hard to explain. So the NMSE normorlized MSE was involved in:

```{r}
(nmse.a1.lm <- mean((lm.predictions.a1 - algae[,'a1'])^2) / mean((mean(algae[,'a1']) - algae[,'a1'])^2))
(nmse.a1.rt <- mean((rt.predictions.a1 - algae[,'a1'])^2) / mean((mean(algae[,'a1']) - algae[,'a1'])^2))
```
The NMSE is smaller, the better the model is, if NMSE is larger than 1, the model is shit.

Use *regr.eval()* to calculate the linear regression model performance parameters. Sample:
```{r}
regr.eval(algae[,"a1"], rt.predictions.a1, train.y = algae[,"a1"])
```

The visualization of the model and predictions in scatterplot:

```{r}
old.par <- par(mfrow = c(1, 2))
plot(lm.predictions.a1, algae[,"a1"], main = "Linear Model", xlab = "Predictions", ylab = "True Values")
abline(0, 1, lty = 2)
plot(rt.predictions.a1, algae[,"a1"], main = "Regression Tree", xlab = "Predictions", ylab = "True Values")
abline(0, 1, lty=2)
par(old.par)
```

Use *ifelse()* to improve model prediction results. Three parameters include logic condition, the value of the function when logic is true, and the value when false. 

```{r}
sensible.lm.predictions.a1 <- ifelse(lm.predictions.a1 < 0, 0, lm.predictions.a1)
regr.eval(algae[,"a1"], lm.predictions.a1, stats = c("mae", "mse"))
regr.eval(algae[,"a1"], sensible.lm.predictions.a1, stats = c("mae", "mse"))

```

Use *experimentComparison()* to do model selection and comparison, three parameters include dataset, models and the parameters in the experiment.

```{r}
cv.rpart <- function(form, train, test, ...){
        m <- rpartXse(form, train, ...)
        p <- predict(m, test)
        mse <- mean((p - resp(form, test))^2)
        c(nmse = mse/mean((mean(resp(form, train)) - resp(form, test))^2))        
}

cv.lm <- function(form, train, test, ...){
        m <- lm(form, train, ...)
        p <- predict(m, test)
        p <- ifelse(p < 0, 0, p)
        mse <- mean((p - resp(form, test))^2)
        c(nmse = mse/mean((mean(resp(form, train)) - resp(form, test))^2))
}

res <- experimentalComparison(
        c(dataset(a1~., clean.algae[, 1:12], 'a1')),
        c(variants('cv.lm'), variants('cv.rpart', se = c(0, 0.5, 1))), cvSettings(3,10,1234))

summary(res)
plot(res)
```

If you want to get any model's parameter, use
```{r}
getVariant("cv.rpart.v1", res)
```
 
For all comparisons within the seven prediction tasks, use
```{r}
DSs <- sapply(names(clean.algae)[12:18],
              function(x,names.attrs){
                      f <- as.formula(paste(x, ' ~.'))
                      dataset(f, clean.algae[,c(names.attrs, x)], x)
              },
              names(clean.algae[1:11]))


res.all <- experimentalComparison(DSs,
                                  c(variants('cv.lm'),
                                    variants('cv.rpart', se=c(0,0.5,1))
                                    ),
                                  cvSettings(5, 10, 1234))

plot(res.all)
bestScores(res.all)

```

Use boosting to choose the best model:
```{r, eval=FALSE}
library(randomForest)
cv.rf <- function(form, train, test, ...){
        m <- randomForest(form, train, ...)
        p <- predict(m, test)
        mse <- mean((p - resp(form, test))^2)
        c(nmse = mse/mean((mean(resp(form, train)) - resp(form, test))^2))
}

res.all <- experimentalComparison(
        DSs,
        c(variants('cv.lm'),
          variants('cv.rpart', se = c(0, 0.5, 1)),
          variants('cv.rf', ntree=c(200, 500, 700))
          ),
        cvSettings(5,10,1234))

bestScores(res.all)

compAnalysis(res.all, against='cv.rf.v3', datasets=c('a1','a2','a4','a6'))

```

To acquire all 7 models:
```{r, eval=FALSE}
bestModelsNames <- sapply(bestScores(res.all),
                          function(x) x['nmse', 'system'])

learners <- c(rf = 'randomForest', rpart='rpartXse')

funcs <- learners[sapply(strsplit(bestModelsNames, '\\.'), function(x) x[2])]

parSetts <- lapply(bestModelsNames, function(x) getVariant(x, res.all) @pars)

bestModels <- list()

#for(a in 1:7){
#        form <- as.formula(paste(names(clean.algae)[11+a],'~.'))
#        bestModels[[a]] <- do.call(funcs[a],
#                                   c(list(form, clean.algae[, c(1:11,11+a)]), parSetts[[a]]))
#}
```